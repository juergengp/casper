#!/bin/bash
#
# ollamaCode - Interactive CLI for Ollama (Claude Code style)
# Version: 2.0.0
# Author: Core.at
# License: MIT
#

set -euo pipefail

# Configuration
VERSION="2.0.0"
CONFIG_DIR="${HOME}/.config/ollamacode"
CONFIG_FILE="${CONFIG_DIR}/config"
HISTORY_FILE="${CONFIG_DIR}/history"
SESSION_DIR="${CONFIG_DIR}/sessions"
CONVERSATION_FILE="${CONFIG_DIR}/conversation.json"

# Find lib directory
if [[ -d "/usr/local/lib/ollamacode" ]]; then
    LIB_DIR="/usr/local/lib/ollamacode"
elif [[ -d "/usr/lib/ollamacode" ]]; then
    LIB_DIR="/usr/lib/ollamacode"
elif [[ -d "$(dirname "$0")/../lib" ]]; then
    LIB_DIR="$(cd "$(dirname "$0")/../lib" && pwd)"
else
    echo "Error: Could not find lib directory"
    exit 1
fi

# Source libraries
source "${LIB_DIR}/system_prompt.sh"
source "${LIB_DIR}/tool_parser.sh"
source "${LIB_DIR}/tool_executor.sh"

# Default settings
DEFAULT_MODEL="coreEchoFlux"
DEFAULT_OLLAMA_HOST="http://localhost:11434"
DEFAULT_TEMPERATURE="0.7"
DEFAULT_MAX_TOKENS="4096"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'
BOLD='\033[1m'

# Initialize configuration
init_config() {
    mkdir -p "${CONFIG_DIR}" "${SESSION_DIR}"

    if [[ ! -f "${CONFIG_FILE}" ]]; then
        cat > "${CONFIG_FILE}" << EOF
# ollamaCode Configuration
MODEL=${DEFAULT_MODEL}
OLLAMA_HOST=${DEFAULT_OLLAMA_HOST}
TEMPERATURE=${DEFAULT_TEMPERATURE}
MAX_TOKENS=${DEFAULT_MAX_TOKENS}
SAFE_MODE=true
AUTO_APPROVE=false
EOF
    fi

    touch "${HISTORY_FILE}"
}

# Load configuration
load_config() {
    if [[ -f "${CONFIG_FILE}" ]]; then
        source "${CONFIG_FILE}"
    fi
}

# Print banner
print_banner() {
    echo -e "${CYAN}${BOLD}"
    cat << 'EOF'
   ____  _ _                       ____          _
  / __ \| | | __ _ _ __ ___   __ _/ ___|___   __| | ___
 | |  | | | |/ _` | '_ ` _ \ / _` | |   / _ \ / _` |/ _ \
 | |__| | | | (_| | | | | | | (_| | |__| (_) | (_| |  __/
  \____/|_|_|\__,_|_| |_| |_|\__,_|\____\___/ \__,_|\___|

EOF
    echo -e "${NC}${BLUE}Interactive CLI for Ollama - Version ${VERSION}${NC}"
    echo -e "${YELLOW}Type 'help' for commands, 'exit' to quit${NC}"
    echo ""
}

# Usage information
usage() {
    cat << EOF
Usage: ollamacode [OPTIONS] [PROMPT]

Interactive CLI for Ollama with Claude Code-like tool calling

OPTIONS:
    -m, --model MODEL       Use specific model (default: ${MODEL})
    -t, --temperature NUM   Set temperature (default: ${TEMPERATURE})
    -s, --system PROMPT     Set custom system prompt
    -a, --auto-approve      Auto-approve all tool executions
    --unsafe                Disable safe mode (allow all commands)
    -v, --version           Show version
    -h, --help              Show this help

INTERACTIVE COMMANDS:
    help                    Show available commands
    models                  List available models
    use MODEL               Switch to different model
    temp NUM                Set temperature
    safe [on|off]           Toggle safe mode
    auto [on|off]           Toggle auto-approve
    clear                   Clear screen
    config                  Show configuration
    exit, quit              Exit ollamacode

EXAMPLES:
    ollamacode                              # Start interactive mode
    ollamacode "List all Python files"      # Single prompt with tools
    ollamacode -m llama3 "Hello"            # Use specific model
    ollamacode -a "Build the project"       # Auto-approve all tools
    ollamacode --unsafe "System update"     # Disable safety checks

EOF
}

# Check if ollama is installed and running
check_ollama() {
    if ! command -v ollama &> /dev/null; then
        echo -e "${RED}âœ— Error: ollama is not installed${NC}"
        echo -e "${YELLOW}Install from: https://ollama.ai${NC}"
        exit 1
    fi

    if ! curl -s "${OLLAMA_HOST}/api/tags" &> /dev/null; then
        echo -e "${RED}âœ— Error: ollama is not running at ${OLLAMA_HOST}${NC}"
        echo -e "${YELLOW}Start with: ollama serve${NC}"
        exit 1
    fi
}

# List available models
list_models() {
    echo -e "${CYAN}${BOLD}Available Models:${NC}"
    ollama list | tail -n +2 | awk '{print "  " $1}' | sort
}

# Show configuration
show_config() {
    echo -e "${CYAN}${BOLD}Current Configuration:${NC}"
    echo -e "  Model:        ${GREEN}${MODEL}${NC}"
    echo -e "  Host:         ${OLLAMA_HOST}"
    echo -e "  Temperature:  ${TEMPERATURE}"
    echo -e "  Max Tokens:   ${MAX_TOKENS}"
    echo -e "  Safe Mode:    ${SAFE_MODE}"
    echo -e "  Auto Approve: ${AUTO_APPROVE}"
    echo -e "  Working Dir:  $(pwd)"
}

# Build conversation context
build_context() {
    local system_prompt=$(get_system_prompt)
    local user_message="$1"

    # Add environment info
    local env_info="
Current Environment:
- Working Directory: $(pwd)
- User: $(whoami)
- Date: $(date '+%Y-%m-%d %H:%M:%S')
- OS: $(uname -s)

User Request: ${user_message}"

    echo "${system_prompt}\n\n${env_info}"
}

# Send prompt to Ollama and get response
send_to_ollama() {
    local prompt="$1"
    local show_thinking="${2:-true}"

    if [[ "$show_thinking" == "true" ]]; then
        echo -e "${BLUE}${BOLD}ðŸ¤” Thinking...${NC}"
        echo ""
    fi

    # Escape special characters for JSON
    local escaped_prompt=$(echo "$prompt" | jq -Rs .)

    # Build JSON payload
    local json_payload=$(cat <<EOF
{
  "model": "${MODEL}",
  "prompt": ${escaped_prompt},
  "stream": false,
  "options": {
    "temperature": ${TEMPERATURE},
    "num_predict": ${MAX_TOKENS}
  }
}
EOF
)

    # Send request
    local response=$(curl -s -X POST "${OLLAMA_HOST}/api/generate" \
        -H "Content-Type: application/json" \
        -d "${json_payload}")

    # Extract and return the response text
    echo "${response}" | jq -r '.response'
}

# Process AI response with tool calling
process_response() {
    local response="$1"
    local iteration="${2:-1}"
    local max_iterations=10

    # Safety check for infinite loops
    if [[ $iteration -gt $max_iterations ]]; then
        echo -e "${RED}âš  Maximum tool calling iterations reached${NC}"
        return 1
    fi

    # Parse tool calls
    local tool_calls_file=$(parse_tool_calls "$response")

    # Get and display response text (non-tool parts)
    local response_text=$(get_response_text "$response")
    if [[ -n "$response_text" ]]; then
        echo -e "${GREEN}${response_text}${NC}"
        echo ""
    fi

    # If no tool calls, we're done
    if [[ "$tool_calls_file" == "[]" ]] || [[ ! -f "$tool_calls_file" ]]; then
        return 0
    fi

    # Count and execute tool calls
    local tool_count=$(count_tool_calls "$tool_calls_file")
    echo -e "${CYAN}${BOLD}ðŸ”§ Executing ${tool_count} tool(s)...${NC}"
    echo ""

    local tool_results=""
    for ((i=0; i<tool_count; i++)); do
        local tool_call=$(extract_tool_call "$tool_calls_file" "$i")
        local tool_name=$(parse_tool_name "$tool_call")

        echo -e "${MAGENTA}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
        echo -e "${MAGENTA}Tool $((i+1))/${tool_count}: ${tool_name}${NC}"
        echo -e "${MAGENTA}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"

        # Execute tool and capture output
        local tool_output
        tool_output=$(execute_tool "$tool_name" "$tool_call" 2>&1)
        local tool_exit_code=$?

        echo "$tool_output"

        # Build tool results summary
        tool_results+="Tool: ${tool_name}\n"
        tool_results+="Exit Code: ${tool_exit_code}\n"
        tool_results+="Output:\n${tool_output}\n\n"
    done

    # Clean up temp file
    rm -f "$tool_calls_file"

    # If there were tool calls, send results back to AI for next iteration
    if [[ -n "$tool_results" ]]; then
        echo -e "${CYAN}${BOLD}ðŸ“Š Tool execution completed. Processing results...${NC}"
        echo ""

        local followup_prompt="Tool execution results:\n\n${tool_results}\n\nBased on these results, provide your analysis or next steps. Only use more tools if absolutely necessary."

        local next_response=$(send_to_ollama "$followup_prompt" false)

        # Recursively process the next response
        process_response "$next_response" $((iteration + 1))
    fi
}

# Interactive mode
interactive_mode() {
    print_banner
    show_config
    echo ""

    while true; do
        echo -ne "${BOLD}${CYAN}You> ${NC}"
        read -r user_input

        [[ -z "${user_input}" ]] && continue

        # Handle commands
        case "${user_input}" in
            "exit"|"quit")
                echo -e "${GREEN}ðŸ‘‹ Goodbye!${NC}"
                exit 0
                ;;
            "help")
                usage
                continue
                ;;
            "models")
                list_models
                continue
                ;;
            "config")
                show_config
                continue
                ;;
            "clear")
                clear
                print_banner
                continue
                ;;
            use\ *)
                MODEL="${user_input#use }"
                echo -e "${GREEN}âœ“ Switched to model: ${MODEL}${NC}"
                continue
                ;;
            temp\ *)
                TEMPERATURE="${user_input#temp }"
                echo -e "${GREEN}âœ“ Temperature set to: ${TEMPERATURE}${NC}"
                continue
                ;;
            safe\ on|safe\ off)
                if [[ "${user_input}" == "safe on" ]]; then
                    SAFE_MODE="true"
                    echo -e "${GREEN}âœ“ Safe mode enabled${NC}"
                else
                    SAFE_MODE="false"
                    echo -e "${YELLOW}âš  Safe mode disabled${NC}"
                fi
                continue
                ;;
            auto\ on|auto\ off)
                if [[ "${user_input}" == "auto on" ]]; then
                    AUTO_APPROVE="true"
                    echo -e "${YELLOW}âš  Auto-approve enabled${NC}"
                else
                    AUTO_APPROVE="false"
                    echo -e "${GREEN}âœ“ Auto-approve disabled${NC}"
                fi
                continue
                ;;
            *)
                # Send to AI with tool support
                echo ""
                local start_time=$(date +%s)

                local context=$(build_context "$user_input")
                local response=$(send_to_ollama "$context")

                process_response "$response"

                local end_time=$(date +%s)
                local duration=$((end_time - start_time))

                echo ""
                echo -e "${MAGENTA}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€${NC}"
                echo -e "${MAGENTA}â± Duration: ${duration}s${NC}"
                echo ""

                # Save to history
                echo "[$(date '+%Y-%m-%d %H:%M:%S')] ${user_input}" >> "${HISTORY_FILE}"
                ;;
        esac
    done
}

# Main function
main() {
    init_config
    load_config

    # Parse arguments
    DIRECT_PROMPT=""
    while [[ $# -gt 0 ]]; do
        case $1 in
            -m|--model)
                MODEL="$2"
                shift 2
                ;;
            -t|--temperature)
                TEMPERATURE="$2"
                shift 2
                ;;
            -s|--system)
                CUSTOM_SYSTEM_PROMPT="$2"
                shift 2
                ;;
            -a|--auto-approve)
                AUTO_APPROVE="true"
                shift
                ;;
            --unsafe)
                SAFE_MODE="false"
                shift
                ;;
            -v|--version)
                echo "ollamaCode version ${VERSION}"
                exit 0
                ;;
            -h|--help)
                usage
                exit 0
                ;;
            *)
                DIRECT_PROMPT="$*"
                break
                ;;
        esac
    done

    check_ollama

    # Handle direct prompt
    if [[ -n "${DIRECT_PROMPT}" ]]; then
        local context=$(build_context "$DIRECT_PROMPT")
        local response=$(send_to_ollama "$context")
        process_response "$response"
        exit 0
    fi

    # Start interactive mode
    interactive_mode
}

# Run main function
main "$@"
