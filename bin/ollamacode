#!/bin/bash
#
# ollamaCode - Interactive CLI for Ollama (Claude Code style)
# Version: 1.0.0
# Author: Core.at
# License: MIT
#

set -euo pipefail

# Configuration
VERSION="1.0.0"
CONFIG_DIR="${HOME}/.config/ollamacode"
CONFIG_FILE="${CONFIG_DIR}/config"
HISTORY_FILE="${CONFIG_DIR}/history"
SESSION_DIR="${CONFIG_DIR}/sessions"

# Default settings
DEFAULT_MODEL="coreEchoFlux"
DEFAULT_OLLAMA_HOST="http://localhost:11434"
DEFAULT_TEMPERATURE="0.7"
DEFAULT_MAX_TOKENS="2048"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color
BOLD='\033[1m'

# Initialize configuration
init_config() {
    mkdir -p "${CONFIG_DIR}" "${SESSION_DIR}"

    if [[ ! -f "${CONFIG_FILE}" ]]; then
        cat > "${CONFIG_FILE}" << EOF
# ollamaCode Configuration
MODEL=${DEFAULT_MODEL}
OLLAMA_HOST=${DEFAULT_OLLAMA_HOST}
TEMPERATURE=${DEFAULT_TEMPERATURE}
MAX_TOKENS=${DEFAULT_MAX_TOKENS}
SYSTEM_PROMPT="You are a helpful AI coding assistant."
EOF
        echo -e "${GREEN}✓ Configuration created at ${CONFIG_FILE}${NC}"
    fi

    touch "${HISTORY_FILE}"
}

# Load configuration
load_config() {
    if [[ -f "${CONFIG_FILE}" ]]; then
        # shellcheck disable=SC1090
        source "${CONFIG_FILE}"
    fi
}

# Print banner
print_banner() {
    echo -e "${CYAN}${BOLD}"
    cat << 'EOF'
   ____  _ _                       ____          _
  / __ \| | | __ _ _ __ ___   __ _/ ___|___   __| | ___
 | |  | | | |/ _` | '_ ` _ \ / _` | |   / _ \ / _` |/ _ \
 | |__| | | | (_| | | | | | | (_| | |__| (_) | (_| |  __/
  \____/|_|_|\__,_|_| |_| |_|\__,_|\____\___/ \__,_|\___|

EOF
    echo -e "${NC}${BLUE}Interactive CLI for Ollama - Version ${VERSION}${NC}"
    echo -e "${YELLOW}Type 'help' for commands, 'exit' to quit${NC}"
    echo ""
}

# Usage information
usage() {
    cat << EOF
Usage: ollamacode [OPTIONS] [PROMPT]

Interactive CLI for Ollama with Claude Code-like features

OPTIONS:
    -m, --model MODEL       Use specific model (default: ${MODEL})
    -t, --temperature NUM   Set temperature (default: ${TEMPERATURE})
    -s, --system PROMPT     Set system prompt
    -f, --file FILE         Read prompt from file
    -c, --config            Show current configuration
    -l, --list              List available models
    -v, --version           Show version
    -h, --help              Show this help

INTERACTIVE COMMANDS:
    help                    Show available commands
    models                  List available models
    use MODEL               Switch to different model
    temp NUM                Set temperature
    system PROMPT           Set system prompt
    history                 Show conversation history
    save [NAME]             Save current session
    load NAME               Load saved session
    clear                   Clear screen
    config                  Show configuration
    exit, quit              Exit ollamacode

EXAMPLES:
    ollamacode                              # Start interactive mode
    ollamacode "Explain Docker"             # Single prompt
    ollamacode -m llama3 "Hello"            # Use specific model
    ollamacode -f prompt.txt                # Read from file
    ollamacode --system "You are DevOps expert" "Setup nginx"

EOF
}

# Check if ollama is installed
check_ollama() {
    if ! command -v ollama &> /dev/null; then
        echo -e "${RED}✗ Error: ollama is not installed${NC}"
        echo -e "${YELLOW}Please install ollama from: https://ollama.ai${NC}"
        exit 1
    fi

    # Check if ollama is running
    if ! curl -s "${OLLAMA_HOST}/api/tags" &> /dev/null; then
        echo -e "${RED}✗ Error: ollama is not running${NC}"
        echo -e "${YELLOW}Start ollama with: ollama serve${NC}"
        exit 1
    fi
}

# List available models
list_models() {
    echo -e "${CYAN}${BOLD}Available Models:${NC}"
    ollama list | tail -n +2 | awk '{print "  " $1}' | sort
}

# Show configuration
show_config() {
    echo -e "${CYAN}${BOLD}Current Configuration:${NC}"
    echo -e "  Model:        ${GREEN}${MODEL}${NC}"
    echo -e "  Host:         ${OLLAMA_HOST}"
    echo -e "  Temperature:  ${TEMPERATURE}"
    echo -e "  Max Tokens:   ${MAX_TOKENS}"
    echo -e "  Config File:  ${CONFIG_FILE}"
    echo -e "  History:      ${HISTORY_FILE}"
}

# Send prompt to Ollama
send_prompt() {
    local prompt="$1"
    local show_stats="${2:-false}"

    echo -e "${BLUE}${BOLD}Thinking...${NC}"
    echo ""

    local start_time=$(date +%s)

    # Build JSON payload
    local json_payload=$(cat <<EOF
{
  "model": "${MODEL}",
  "prompt": "${prompt}",
  "stream": false,
  "options": {
    "temperature": ${TEMPERATURE},
    "num_predict": ${MAX_TOKENS}
  }
}
EOF
)

    # Send request to Ollama
    local response=$(curl -s -X POST "${OLLAMA_HOST}/api/generate" \
        -H "Content-Type: application/json" \
        -d "${json_payload}")

    local end_time=$(date +%s)
    local duration=$((end_time - start_time))

    # Extract response
    local ai_response=$(echo "${response}" | jq -r '.response')
    local total_duration=$(echo "${response}" | jq -r '.total_duration // 0')
    local eval_count=$(echo "${response}" | jq -r '.eval_count // 0')

    # Print response
    echo -e "${GREEN}${ai_response}${NC}"
    echo ""

    # Show stats if requested
    if [[ "${show_stats}" == "true" ]]; then
        local tokens_per_sec=0
        if [[ ${duration} -gt 0 ]] && [[ ${eval_count} -gt 0 ]]; then
            tokens_per_sec=$(awk "BEGIN {printf \"%.2f\", ${eval_count}/${duration}}")
        fi

        echo -e "${MAGENTA}───────────────────────────────────────${NC}"
        echo -e "${MAGENTA}Stats: ${duration}s | ${eval_count} tokens | ${tokens_per_sec} tok/s${NC}"
    fi

    # Save to history
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] ${prompt}" >> "${HISTORY_FILE}"
}

# Interactive mode
interactive_mode() {
    print_banner
    show_config
    echo ""

    local conversation_history=()

    while true; do
        echo -ne "${BOLD}${CYAN}You>${NC} "
        read -r user_input

        # Skip empty input
        [[ -z "${user_input}" ]] && continue

        # Handle commands
        case "${user_input}" in
            "exit"|"quit")
                echo -e "${GREEN}Goodbye!${NC}"
                exit 0
                ;;
            "help")
                usage
                continue
                ;;
            "models")
                list_models
                continue
                ;;
            "config")
                show_config
                continue
                ;;
            "clear")
                clear
                print_banner
                continue
                ;;
            "history")
                echo -e "${CYAN}${BOLD}Conversation History:${NC}"
                tail -20 "${HISTORY_FILE}"
                continue
                ;;
            use\ *)
                MODEL="${user_input#use }"
                echo -e "${GREEN}✓ Switched to model: ${MODEL}${NC}"
                # Cross-platform sed: macOS requires -i '' while Linux uses -i
                if [[ "$(uname)" == "Darwin" ]]; then
                    sed -i '' "s/^MODEL=.*/MODEL=${MODEL}/" "${CONFIG_FILE}"
                else
                    sed -i "s/^MODEL=.*/MODEL=${MODEL}/" "${CONFIG_FILE}"
                fi
                continue
                ;;
            temp\ *)
                TEMPERATURE="${user_input#temp }"
                echo -e "${GREEN}✓ Temperature set to: ${TEMPERATURE}${NC}"
                # Cross-platform sed: macOS requires -i '' while Linux uses -i
                if [[ "$(uname)" == "Darwin" ]]; then
                    sed -i '' "s/^TEMPERATURE=.*/TEMPERATURE=${TEMPERATURE}/" "${CONFIG_FILE}"
                else
                    sed -i "s/^TEMPERATURE=.*/TEMPERATURE=${TEMPERATURE}/" "${CONFIG_FILE}"
                fi
                continue
                ;;
            system\ *)
                SYSTEM_PROMPT="${user_input#system }"
                echo -e "${GREEN}✓ System prompt updated${NC}"
                continue
                ;;
            *)
                send_prompt "${user_input}" "true"
                ;;
        esac
    done
}

# Main function
main() {
    init_config
    load_config

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -m|--model)
                MODEL="$2"
                shift 2
                ;;
            -t|--temperature)
                TEMPERATURE="$2"
                shift 2
                ;;
            -s|--system)
                SYSTEM_PROMPT="$2"
                shift 2
                ;;
            -f|--file)
                PROMPT_FILE="$2"
                shift 2
                ;;
            -c|--config)
                show_config
                exit 0
                ;;
            -l|--list)
                list_models
                exit 0
                ;;
            -v|--version)
                echo "ollamaCode version ${VERSION}"
                exit 0
                ;;
            -h|--help)
                usage
                exit 0
                ;;
            *)
                DIRECT_PROMPT="$*"
                break
                ;;
        esac
    done

    check_ollama

    # Handle direct prompt or file input
    if [[ -n "${DIRECT_PROMPT:-}" ]]; then
        send_prompt "${DIRECT_PROMPT}" "false"
        exit 0
    elif [[ -n "${PROMPT_FILE:-}" ]]; then
        if [[ -f "${PROMPT_FILE}" ]]; then
            PROMPT_CONTENT=$(cat "${PROMPT_FILE}")
            send_prompt "${PROMPT_CONTENT}" "false"
            exit 0
        else
            echo -e "${RED}✗ File not found: ${PROMPT_FILE}${NC}"
            exit 1
        fi
    else
        # Start interactive mode
        interactive_mode
    fi
}

# Run main function
main "$@"
